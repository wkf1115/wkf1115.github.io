<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://wkf1115.github.io</id>
    <title>WKF1115</title>
    <updated>2020-04-21T09:38:32.735Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://wkf1115.github.io"/>
    <link rel="self" href="https://wkf1115.github.io/atom.xml"/>
    <subtitle>快乐咸鱼🐟每一天</subtitle>
    <logo>https://wkf1115.github.io/images/avatar.png</logo>
    <icon>https://wkf1115.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, WKF1115</rights>
    <entry>
        <title type="html"><![CDATA[Java并发编程艺术第四章学习笔记📒]]></title>
        <id>https://wkf1115.github.io/post/java-bing-fa-bian-cheng-yi-zhu-di-si-zhang-xue-xi-bi-ji/</id>
        <link href="https://wkf1115.github.io/post/java-bing-fa-bian-cheng-yi-zhu-di-si-zhang-xue-xi-bi-ji/">
        </link>
        <updated>2020-04-21T09:06:58.000Z</updated>
        <content type="html"><![CDATA[<p>第四章：JAVA并发编程基础</p>
<pre><code>实际上JAVA天生是一个多线程语言，在运行mian方法的时候就会执行多个线程。多线程带来了许多好处，更多的处理器核心，更快的响应时间，更好的编程模型。
</code></pre>
<p>1.线程优先级：</p>
<pre><code>    现代操作系统分配内存时会分出一个个时间片，每一个线程使用不等数量的时间片，线程时间片用完了就会发生线程调度。

    线程优先级就是线程需要多少处理器的资源。

    首先Java线程是通过一个名字叫做priority的整形成员变量来控制优先级，优先级的范围是1～10。
    可以通过setPriority(int)方法设置优先级，默认优先级是5，优先级高的线程时间片要多于低的。

    🍉频繁阻塞（休眠或者I/O操作）的线程需要设置较高的优先级，偏重计算（需要较多cpu时间或者偏运算）的线程需要较低的优先级。
    🍉这样可以避免cpu资源被偏重计算的线程独占。
    
    🍎线程优先性的设置，在很多实际jvm环境中都不会生效，所以这条仅供参考。
</code></pre>
<p>2.线程的状态：</p>
<pre><code>    线程在生命周期中有6种状态：
        
        1.NEW：初始状态，线程刚刚被构建，还没有调用start()方法。
        
        2.RUNNABLE：运行状态，操作系统的就绪和运行两种状态都被称作运行状态。
        
        3.BLOCKED：阻塞状态，表示被锁阻塞。

        4.WAITING：等待状态，表示该线程需要其它线程作出一些特定动作。
        
        5.TIME_WAITING：超时等待，不同于WAITING，他可以在指定时间内自行返回。

        6.TERMINATED：终止状态，线程执行完毕。
    
    线程的运行流程：线程创建后会调用start()方法开始运行，线程执行wait()方法后，线程会进入等待状态。等待状态需要其他线程的通知才能变回运行状态，
                    超时等待时间到达将会返回到运行状态，线程在没有获取锁的时候会进入阻塞状态，线程在执行Runnable的run()方法后会进入终止状态。
</code></pre>
<p>3.Daemon线程：</p>
<pre><code>        🍉Daemon线程主要用于支持程序后台调度和支持性工作，当JVM中不存在非Daemon线程的时候，JVM会退出。
</code></pre>
<p>4.线程的创建：</p>
<pre><code>        一个新线程的构造是由其parent线程进行空间分配的，它继承了parent线程是否为Daemon，优先级，加载资源的contextClassLoader还有可继承的ThreadLocal，
        这个线程还会有唯一的ID。
        
        创建结束之后调用start()方法就可以启动线程，只要线程规划器空闲，应该立即启动该线程。
</code></pre>
<p>5.线程的终止：</p>
<pre><code>        线程的中断操作可以作为一种简便的线程间交互方式，同时也可在线程内设置变量来控制线程的终止。
</code></pre>
<p>6.synchronized的方式：</p>
<pre><code>        任意线程对Object的访问需要经过它的Monitor（监视器），线程首先对监视器执行Monitor.Enter()方法，如果Enter失败，线程会进入一个同步队列。
        线程在释放锁的时候，会执行Monitor.exit()方法，同步队列检测到exit方法被调用时，队头的线程会执行出队操作并执行Monitor.Enter()方法。
</code></pre>
<p>7.wait()和notify()：</p>
<pre><code>        1.wait()和notify(),notifyAll()在调用时需要给先获取到锁。
        
        2.调用wait()方法后，线程状态变为waiting，并加入到同步队列中。
    
        3.notify调用后，线程不会立即从wait返回，需要调用notify的线程释放锁后，wait线程获取到锁，才有可能返回。
            🍉（notify方法将一个wait线程从等待队列中移到同步队列中，notifyAll将等待队列中全部的线程都移到同步队列，被移动的线程由waiting变为blocked状态）
￼
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://wkf1115.github.io/post-images/1587460118713.png" alt="" loading="lazy"></figure>
<p>8.等待/通知的经典范式：</p>
<pre><code>        等待方：
        
            1.获取对象的锁
            2.如果条件不满足，调用对象的wait()方法，如果条件满足停止wait()
        
        通知方：
            
            1.获得对象的锁
            2.改变条件
            3.通知所有等待的线程
</code></pre>
<p>9.thread.join()的使用：</p>
<pre><code>        如果一个线程A执行了B.join()语句，那么A等待B线程结束之后才会从B.join()返回（继续执行）。
        从源码角度来看，join()的实现使用了等待/通知的经典范式—加锁，循环，和处理逻辑，只不过条件变为了线程是否活着。
</code></pre>
<p>10.ThreadLocal的使用：</p>
<pre><code>        ThreadLocal是线程变量，是以ThreadLocal对象为键，任意对象为值的储存结构。这个结构是附带在线程上的。也就是一个线程可以根据一个ThreadLocal对象查询				该线程的某些信息。

        springAOP中可以用到此类方法。因为这个变量可以将方法拆分开，中间插入其他方法，通过变量来传递信息。</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java并发编程艺术第三章学习笔记📒]]></title>
        <id>https://wkf1115.github.io/post/java-bing-fa-bian-cheng-yi-zhu-di-san-zhang-xue-xi-bi-ji/</id>
        <link href="https://wkf1115.github.io/post/java-bing-fa-bian-cheng-yi-zhu-di-san-zhang-xue-xi-bi-ji/">
        </link>
        <updated>2020-04-21T09:02:00.000Z</updated>
        <content type="html"><![CDATA[<p>第三章：java内存模型（JMM）</p>
<pre><code>为了了解内存模型的基础，我们就要明白线程拥有两种通信机制：共享内存和消息传递
共享内存通过主程序的内存来实现线程间的通信，所以它是隐式通信，同时它需要程序员显式的制定的执行顺序，也叫显式同步。
消息传递通过线程之间传递消息来实现线程间的通信，所以它是显式通信，同时它不需要程序员人为指定执行顺序，因为消息发送必须在消息接收前，也叫隐式同步。
</code></pre>
<p>1.JMM的抽象结构：</p>
<pre><code>    java中的共享变量（实例域，静态域，数组元素）都储存在堆内存中，这部分内存是线程共享的，也就是说他们需要受到JMM的管理。
    
    从抽象的角度看，每个线程拥有一个私有的本地内存，线程之间的共享变量都存在主内存中，三者之间的关系类似cpu与高速缓存与主内存的关系。

    🍉本地内存与主内存之间的关系通过JMM进行控制
    🍉线程之间的通信就类似于上一章的cpu，高速缓存，主内存模型中cpu与cpu之间的通信。（必须经过主内存）
    🍉当然，这仅仅为抽象结构，实际并不存在。
</code></pre>
<p>2.🍊源代码和指令序列的重排序：（🍊本章重点，JMM离不开重排序）</p>
<pre><code>    在执行程序时，我们写的源代码为了发挥最大性能，常常会被重排序。
        
        1.编译器重排序：

                编译器在不影响单线程语义的情况下，会重新安排语句的执行顺序。
        
        2.指令级并行的重排序：
                
                cpu在机器指令不存在数据依赖性的情况，可以改变机器指令的执行顺序。

        3.内存系统的重排序：
                
                对cpu的读取和储存操作进行修改，修改读写顺序。
        
        🍉对以上的2，3操作，JMM会对出现问题的重排序进行监管，为了不让程序出现排序错误，会在生成指令序列时在序列中插入内存屏障，通过屏障来禁止重排序
</code></pre>
<p>3.屏障类型：</p>
<pre><code>    1.LoadLoad屏障，确保前面的装载（读取）命令先于后面的装载（读取）命令
    2.StoreStore屏障，确保前面的存储（向主内存中刷入缓存区数据）先于后面的存储（向主内存中刷入缓存区数据）
    3.LoadStore屏障，确保前面的装载先于后面的存储
    4.StoreLoad屏障，这个屏障比较特殊，它会使前面的内存访问指令（存储和读取）先于后面的内存访问指令（存储和读取）
</code></pre>
<p>4.happens-before：<br>
这是一个规则，代表着两个操作之间的关系。</p>
<pre><code>    1.一个线程中的每个操作happens-before于该线程中的任意后续操作（程序顺序规则）

    2.对于一个锁的解锁，happens-before于随后对这个锁的加锁，也就是先解锁再加锁（监视器锁规则）

    3.对于一个volatile域的写，happens-before于任意后续对这个volatile域的读。（volatile变量规则）

    🍎4.如果 A happens-before  B，B happens-before C，那么 A happens-before C （传递性）
    
    5.如果线程A执行ThreadB.start()操作，那么A线程的ThreadB.start()操作happens-before线程B的任意操作（start（）规则）
    
    6.如果线程A执行ThreadB.join()操作并成功返回，那么线程B的任意操作happens-beforeA线程的ThreadB.join()操作成功返回(join()规则)
    
    那么我们定义的happens-before规则和JMM有什么关系呢。

    🍉JMM在底层定义了多个cpu重排序规则，JMM进而通过规则禁止某个重排序（通过屏障），而这个屏障呈现出来就是一个一个的happens-before规则。
</code></pre>
<p>5.数据依赖性：</p>
<pre><code>    数据依赖性是什么呢，它是代表了两个操作如果被重排序了会影响结果。
    
    如果两个操作使用了同一个变量，且这两个操作中有一个为写操作，那么这两个操作就有了数据依赖性。
    
    例如写后读，写后写，读后写。这三个操作只要被重写，执行结果就会改变。

    🍉在JMM排序时，会遵循数据依赖性规则，具有数据依赖的两个操作不会被重排序。要注意的是这仅仅是在单个线程中遵循的规则，线程之间并不遵守。
</code></pre>
<p>6.as-if-serial:</p>
<pre><code>    as-if-serial语义就是不管怎么重排序，编译器和cpu都不能对存在数据依赖性的操作作重排序，这样保证了程序执行的结果正确。
    🍉它为程序员营造了一个幻觉：单线程程序是按照程序的顺序来执行的。
    🍎程序遇到if条件，cpu会执行猜测操作，即先计算if方法体的内容，将结果保存到ROB中，如果if条件为真，再把计算的值赋给if方法体内的变量，这也是一种变相的重排序。在单线程内不会影响程序结果。但在并发情况下会出现结果错误。
</code></pre>
<p>7.顺序一致性内存模型（被同步过的程序）：</p>
<pre><code>    顺序一致性内存模型是被理想化的参考模型，程序员在实际操作的时候可以讲程序的执行顺序看作顺序一致性内存模型。
    
    🍉它的顺序为每个线程是原子性的，每个线程内部操作执行顺序是和程序顺序相同的。
    
    🍉实际上JMM采用的内存模型为保证线程原子性的同时对线程内部进行适当的重排序操作。
</code></pre>
<p>8.没有同步过的程序：</p>
<pre><code>    JMM保证未同步的程序读取值一定是被初始化过的，要么为之前写入的，要么为默认值。
    
    🍉JMM不保证多线程和单线程之间的执行顺序相同，同时也不保证64位long型和double型的写操作具有原子性
    （因为64位数据实际上写入不是一步完成的，可能会被重排序），没有同步过的单线程是不用担心的，因为数据依赖性的关系不会出现数据丢失。
</code></pre>
<p>9.volatile特性在内存上的理解：</p>
<pre><code>    🍉要理解volatile特性，可以把volatile变量中读/写的操作看成对读/写操作加了同一个锁。

    🍉当然，volatile实际上和锁没啥关系，上一章讲过，volatile在cpu层面上是直接使用lock命令，虽然和锁底层实现机理相同（锁的CAS操作也调用了lock）

    🍉此处使用锁来做类比也是为了更好的理解volatile，在内存特性上，volatile的读和锁的获取有相同语义，volatile的写和锁的释放有相同语义（当volatile写入的时候，JMM会把该线程的本地内存刷新到主内存，当volatile读入的时候，会把线程本地内存置为无效，强制从主内存读取数据），

    🍎volatile读写操作不止有原子性，也有先后顺序，最后的写操作永远先于任意的读操作（上章的cpu的总线锁和缓存锁就是为此存在的）

    🍑书中写锁的内存可见性推导出volatile的内存可见性，容易被误解成volatile实现了锁的happens-before规则，但我认为书中原意是描写了volatile和锁的相似性
        （两者都是基于cpu的内存锁和缓存锁）
</code></pre>
<p>10.JMM如何实现volatile的内存语义：</p>
<pre><code>    JMM操作策略：

        1.volatile写永远在后面（volatile写入之前的操作不会被重排序到volatile后面）当第二个操作是volatile写的时候，第一个操作无论是什么都不能重排序
        2.volatile读一定在前面（volatile读之后的操作不会被重排序到volatile前面）当第一个操作是volatile读的时候，第二个操作无论是什么都不能重排序
        3.volatile写+读也不能排序，当第一个操作是volatile写，第二个操作是volatile读的时候，不能重排序
    
    JMM实际操作情况：

        🍉volatile写前面插入StoreStore屏障（禁止把v写和前面的写重排序），volatile写后面插入StoreLoad屏障（比较特殊禁止全部重排序，因为下面有可能有v读）
        🍉volatile读前面插入LoadLoad屏障（禁止把前面的读和v读重排序），volatile读后面插入LoadStore屏障（禁止下面的写和v读重排序）
        
        JMM的屏障操作可能不是最高效的，但十分保守。
        🍎实际操作时JMM可以根据具体情况省略屏障，比如前后并没有读/写操作，这时屏障可以被省略来节省处理器内存开支

    JMM屏障的发展：
        
        事实上在JSR-133以前的java内存模型中，JMM允许v变量读写和普通读写的重排序，这样会导致不存在数据依赖的值（如v变量和普通变量之间）发生数据丢失。为了增强volatile的内存语义，专家组增加了上面写的那些屏障。直接导致了volatile的读写有了加锁和撤销锁的语义
        
    🍎volatile只能保证数据的读写是原子性的，因为volatile就是强制让线程去主内存获取数据。例子：如果两个线程同时运行i++，同时获取i=0，那么结果还是1，并没有保证线程的安全性，线程有时可能并没有更新数据，线程就被切换了。

    🍊重点：JMM实现volatile实际上是运用底层lock指令，lock指令中包含了cpu屏障技术，lock指令保证volatile读写操作的原子性和顺序，
            屏障技术仅仅阻止了重排序，但并不能阻止复杂操作的数据读取混乱，例如同时运行i++。

    🍊任何带有lock前缀的指令以及CPUID等指令都有内存屏障的作用。
</code></pre>
<p>11.锁特性在内存上的理解：</p>
<pre><code>    实际上

        🍉当线程释放锁时，JMM会将线程的本地内存立刻刷新到主内存中（参照上一章的图片），其实线程释放锁就是volatile的写入（上面把锁和volatile一起讲解，现在能看明白了吧）。
    
        🍉当线程获得锁时，JMM会把线程对应的本地内存置无效，并去主内存中读取数值，其实线程获得锁就是volatile的读取。
    
    🍎线程1释放锁—线程2获取锁的过程就是线程1向线程2发送信息的过程（通过主内存）
</code></pre>
<p>12.JMM如何实现锁：</p>
<pre><code>    分析java中一个加锁类ReentrantLock的源代码：

        它调用一个lock()方法获取锁，unlock()方法释放锁。
    
        🍉这个类的实现依赖于AQS（java同步器框架），原理为AQS会维护一个整形的volatile变量state。这个类分为公平锁和非公平锁

        公平锁：在获取锁时首先读volatile变量，这时共享变量只有当前获取锁的线程可以看到。
                    释放锁时写volatile变量，并且使共享变量对其他获取锁的线程可见。

        非公平锁：释放锁时和公平锁操作相同。
                    在获取锁的时候使用CAS方法更新state变量，CAS同时具有volatile读和写的内存语义。
    
    🍎通过这个类可以分析出，java中锁处理具有两种方式：
            1.通过volatile读写的内存语义上锁
            2.利用CAS进行上锁（CAS附带volatile的语义）	
</code></pre>
<p>13.CAS是如何实现的：</p>
<pre><code>        首先，CAS同时具有volatile读和写的内存语义
        
        比较和交换（Conmpare And Swap）是用于实现多线程同步的原子指令。 它将内存位置的内容与给定值进行比较，只有在相同的情况下，将该内存位置的内容修改为新的给定值。
        
        CAS实际上是调用本地处理器中的C++代码，如果程序在多处理器上运行，c++代码执行带有lock前缀的cmpxchg。如果在单处理器上运行，就会省略lock前缀。

        🍉在同一时刻，只会有一个线程更新数据成功
</code></pre>
<p>14.concurrent包的实现：</p>
<pre><code>    由于java的CAS和volatile运用了cpu上的原子指令，并且他们可以实现线程之间的通信，所以concurrent的通用化实现模式就诞生了。

    实现模式：声明变量为volatile，使用CAS实现线程之间的同步，之后使用volatile的读和写还有CAS所具有的v内存语义实现线程的通信。

    🍊🍊🍊重点：volatile只能保证数据读写的原子性，涉及到复杂操作，volatile并不能保证，所以这里使用CAS去更新。因为CAS每次更新前都要比较。
    
    整体来看，concurrent包使用了底层CAS和volatile实现了AQS，非阻塞数据结构和原子变量类，进而实现其他高层类。
</code></pre>
<p>15.final在内存上的理解：</p>
<pre><code>    1.final的重排序规则：
        
        1.这两个操作不能重排序，对一个对象的final变量的写入———————把这个对象的引用进行赋值给另一个引用
        2.这两个操作不能重排序，初次读一个包含final变量的引用———————初次读这个final变量
        
        下面对这两个重排序禁止规则详细介绍：
            
            1.final写的重排序：
                JMM禁止把构造函数中的写final变量操作排序到构造函数外面。

                🍉具体实现操作是JMM会在final变量写操作之后，构造函数结束的return之前，插入一个StoreStore屏障。这样就保证了构造函数执行完成前，final数据一定被初始化了。
                
                🍉像不是final的变量，在构造函数中赋值写入的操作可能就会被重排序到读取的后面，导致数据丢失。
        
            2.final读的重排序：
                JMM禁止读对象引用和读对象的final值重排序
        
                （这两个操作实际上具有间接依赖关系，大部分处理器都不会重排序他们，但少部分处理器会重排序，这个规则就是为此存在的）

        🍎引用类型的final变量，和基本数据类型的重排序规则差不多，但是引用内部的数据初始化可能并不正确，
                比如数组类型，可能只能初始化大小，内部的值是否初始化不一定，因为会被重排序。

        🍉还有一点，具有final变量的类，构造方法中不能出现this，这样会导致“溢出”，因为在构造方法结束之前就初始化了此对象，这样会导致屏障不能得到正确的使用。
</code></pre>
<p>16.final在cpu中的实现：</p>
<pre><code>    🍉final实际上就是在final写后和构造的return前插入StoreStore屏障，在final读前插入LoadLoad屏障。
    
    🍉在X86处理器中，因为X86不会对写-写作重排序，所以会忽略StoreStore屏障，同时final读对象引用实际上具有间接依赖关系，X86也不会处理这个操作。
            发现了吗，实际上X86对final没有进行任何操作。
    
    🍉现在，final修饰的变量，只要保证被正确构造，在任意线程就能看到这个被构造函数初始化之后的值。
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java并发编程艺术第二章学习笔记📒]]></title>
        <id>https://wkf1115.github.io/post/java-bing-fa-bian-cheng-di-er-zhang-xue-xi-bi-ji/</id>
        <link href="https://wkf1115.github.io/post/java-bing-fa-bian-cheng-di-er-zhang-xue-xi-bi-ji/">
        </link>
        <updated>2020-04-21T07:44:57.000Z</updated>
        <content type="html"><![CDATA[<p>第二章：java并发机制的底层实现原理</p>
<p>核心观点：java源代码在编译后会变为java字节码，字节码被类加载器加载到jvm中，jvm执行字节码，最终转化为汇编指令在cpu上执行。<br>
所以在讨论并发时与cpu和jvm是分不开的。</p>
<p>1.volatile</p>
<pre><code>    首先volatile是轻量级的synchronized，它主要保证  共享变量（也就是主内存的变量） 的可见性（其他线程能否读到这个值）。
    它没有加锁，没有线程上下文的切换，所以它的执行成本很低。
	
    1.定义：
        java语言允许线程访问共享变量，为了确保共享变量能够及时的更新，所以使用了volatile。
        换句话说，如果一个字段是volatile，那么JMM会确保所有线程使用该变量的值都是一致的。

    2.底层实现：
        在汇编层面，volatile实际上是使用了Lock前缀的指令，这个Lock指令实际上是
        将缓存行中的指定数据写回到系统内存中，同时使其他cpu中缓存行中该数据的内存地址无效。

        为了说明这个问题，我们使用图片模拟cpu（线程）读写数据的情况。
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://wkf1115.github.io/post-images/1587459567354.png" alt="" loading="lazy"></figure>
<pre><code>        首先，cpu读取数据是从缓存行进行读取，同时缓存行与主内存进行数据交互，这样能够提升程序的运行速度，在单cpu（单线程）时，这样的运行方式没有问题。
        多cpu时，由于缓存行与主内存数据更新不及时，会导致数据更新丢失。
        例如：java程序启动时1线程和2线程从主内存中将a  = 1的地址读取到各自的缓存行。1线程中a++，2线程中缓存行中的a并没有变化，这就导致了数据更新丢失。
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://wkf1115.github.io/post-images/1587459682774.png" alt="" loading="lazy"></figure>
<pre><code>    ￼
        而volatile数据在更新时会向主内存写入数据，同时强制其他cpu读取数据A时去主内存中读取。
        这样就保证了数据A更新的原子性。

        🍉把处理器缓存中数据写到内存中，以前的处理器采用总线锁定，现在大部分处理器会采用缓存锁定。锁定也是为了保证操作的原子性

        🍉处理器通常使用嗅探技术来保证不同处理器之间数据在总线上的一致性。例如，如果嗅探到一个处理器来检测其他处理器打算改写内存地址，而这个地址处于共享状态，那么处理器会使它的缓存行无效。在下次强制执行缓存行填充（这就是缓存锁的大致原理）
        
    3.神奇的优化
        JDK7中LinkTransferQueue队列类中将想要储存的变量扩充了15个变量，一个变量4个字节，算上本身有64个字节。那么为什么扩充字节反而提高了速度呢？

        因为在很多处理器中高速缓存行是64个字节，如果头尾节点不足64字节大小，那么数据读取到高速缓存行时会将头尾节点读取到同一缓存行中，我们也清楚，队列出队和入队需要对头尾节点频繁操作，如果头节点被改变，根据volatile的数据一致性，cpu会锁定对应的缓存行。这会导致其他cpu无法访问尾节点拖慢运行速度。
</code></pre>
<p>2.synchronized</p>
<pre><code>    首先，对synchronized的运用实际上就是对锁的运用，java中任何一个对象都可以作为锁。

    1.java对象头：
        对象是否为锁的信息实际上是存在java对象头中的Mark Word里面，包括它的锁的类型。

    2.锁的类型：
        偏向锁-&gt;轻量级锁-&gt;重量级锁

        1.偏向锁：
                在大部分情况下，程序中的锁总是由同一线程使用，如果这时重复的加锁解锁那么程序效率会浪费，为了让线程以更低的代价使用锁，偏向锁诞生了。
                那么偏向锁是什么呢？

                当一个线程访问带锁的同步块并试图获取锁时，会在对象头中储存线程ID，以后在线程使用锁时实际验证对象头中的线程ID是否和自己的ID相同即可。
                偏向锁实际上是出现线程竞争时才会发生变化。如果1线程持有偏向锁，此时2线程想要获取偏向锁，那么到下一次全局安全点时（此时没有java字节码执行），1线程会被暂停。如果1线程依然活着，要么2线程获取到偏向锁，要么偏向锁会升级。之后暂停的1线程会被唤醒。
            
            🍉综上所述：偏向锁只是通过锁验证线程的ID，省时省力。

        2.轻量级锁：
            
            取锁过程：
                1.线程取锁时，会复制锁中对象头的mark word到线程的栈帧（线程中储存锁记录的空间）中
                2.之后线程会使用CAS尝试把锁中对象头的mark word改为指向此线程的栈帧（锁记录）的指针
                2.1.操作成功，代表此时锁没有线程使用。
                2.2.操作失败，代表这个锁已被修改（CAS没有通过），此时由于是轻量级锁，此线程会选择进行自旋等待锁被释放，自旋一定次数会膨胀。
                🍉持有锁的线程会把程序体执行完成再进行解锁。
                🍉这里提到的自旋实际上就是等待一段时间，没有锁线程等待有锁的线程释放锁，等一会可以，如果时间太久了那么等待中的线程不干了就会掀桌。表现为进行膨胀，将锁升级为重量级。

            🍍——————————中间过程为拿到锁的线程执行方法体的过程——————————🍍

            解锁过程：
                1.轻量级锁解锁时，持有锁的线程会使用CAS把线程栈帧锁记录中的Mark Word副本换回锁中的对象头中
                1.1成功代表没有竞争。这个锁依然为轻量级。
                1.2失败了还说啥，锁膨胀为重量级，剩下的线程进行下一轮抢锁大战。

            🍎如果锁在程序体执行过程中锁膨胀了，其他想要抢这个锁的线程都会进入阻塞状态直到锁被释放	

        3.重量级锁：
            
            真正的synchronized，直接将没有锁的线程进行阻塞，直到锁被释放。

        三个锁的优缺点：

            1.偏向锁：
                优点：我们偏向锁，一天到晚除了快就是快，没别的。因为加锁解锁根本不需要额外的消耗，就是往对象头中加一串ID。
                缺点：如果这个锁被别人竞争了，那么锁进行撤销的过程需要额外的消耗。
                
                🍉一个线程访问的环境就用偏向锁

            2.轻量级锁：
                优点：竞争的线程实际上在自旋，如果锁被释放了，线程马上可以补上，响应速度相比重量级锁的阻塞线程要快。
                缺点：如果线程一直不放锁，那么自旋会消耗不必要的cpu。
                
                🍉追求响应时间，同步块执行速度快的时候用轻量级锁
            
            3.重量级锁：
                优点：和轻量级锁相比不自旋，就不会消耗cpu。
                缺点：线程阻塞，相应（再启动）时间缓慢。
                
                🍉追求吞吐量，同步块执行速度较长的时候用重量级锁
</code></pre>
<p>3.原子操作的实现原理</p>
<pre><code>    1.处理器层面实现原子操作：
        
        首先，处理器的基本内存操作都是原子性的，从系统内存读取或者写入一个字节都是原子的，也就是说当一个处理器读写一个字节时，其他处理器不能访问这个字节的内存地址。
        但是，复杂的内存操作处理器就不能自动保证了，例如跨总线宽度，多个缓存行，跨页表的操作。这时需要两个机制：总线锁和缓存锁。
        
        1.总线锁：
            就是上面volatile中提到的总线锁，使用汇编中的Lock命令触发cpu的LOCK#信号，当一个cpu输出此信号时，其他cpu会被阻塞，该cpu独享主内存，处理完该数据后其他cpu才会被恢复，这就保证了数据的原子性。但是同时，其他cpu被阻塞会导致不必要的开销。目前cpu会在某些时候使用缓存锁替代总线锁。
        
        2.缓存锁：
            由于cpu是和内部的高速缓存进行数据交互（见volatile章节的图），那么cpu1在对内部的高速缓存中的数据A进行操作时，cpu的嗅探机制会触发，同时缓存了数据A的cpu2会强制让其高速缓存内部的数据A地址失效，使其重新去主内存中获取数据A。
        
        🍉缓存锁虽好，但对某些不能存在缓存中的数据不起作用，因为人家压根就不在缓存里，并且，对跨缓存行的数据也不起作用。

    🍎我们可以看到，其实volatile就是完全利用了cpu的原子操作，实际上就是调用了Lock命令（里面包含了屏障，下一章会讲），cpu收到Lock命令后可以选择总线锁，也可以选择缓存锁。

    2.java中实现原子操作：
        
            1.循环CAS：
            
            java中的CAS操作实际上是利用了cpu中的CMPXCHG指令实现的，就是比较这个值有没有发生变化。在java中有java内置的原子操作类（AtomicBoolean，AtomicInteger，AtomicLong），这些类都使用了循环CAS。并且，上面提到的轻量级锁的自旋就是循环CAS
            
            🍉同时循环CAS也有一些问题，例如常见的ABA问题，指的是一个值变了两次，但好像没有变化一样。这样CAS就检查不出变化。解决ABA问题的思路就是加上一个值（比如版本号），在值每次变化时都进行自增，这样可以保证值变化的唯一性。从JDK1.5开始，Atomic包中提供了解决ABA问题的类。
            
            🍉第二个问题也是轻量级锁的问题，那就是自旋循环CAS带来的性能开销有点大。这个问题就要斟酌看待了。使用处理器的pause指令对循环时间进行控制。

        2.各种各样的锁：
                
            偏向锁，轻量级锁，重量级锁。这三个锁没啥好说的，上面有详细的讲。
            
    🍎我们到这里可以了解到，无论是CAS，轻量级锁，重量级锁，还是CMPXCHG，都是基于cpu的Lock指令
    🍎lock指令有三个作用：第一个就是锁住总线（总线锁和缓存锁），第二个是禁止一部分重排序，第三个是将缓存数据刷到主内存中
    🍎注意，CMPXCHG也是基于总线锁和缓存锁的，不然CAS不会有原子性
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java并发编程艺术第一章学习笔记📒]]></title>
        <id>https://wkf1115.github.io/post/java-bing-fa-bian-cheng-yi-zhu-di-yi-zhang-xue-xi-bi-ji/</id>
        <link href="https://wkf1115.github.io/post/java-bing-fa-bian-cheng-yi-zhu-di-yi-zhang-xue-xi-bi-ji/">
        </link>
        <updated>2020-04-21T07:36:45.000Z</updated>
        <content type="html"><![CDATA[<p>第一章：关于多线程的简单介绍</p>
<p>1.多线程中并发执行不一定比串行执行更快，当并发执行累加操作不超过百万次时，速度会比串行执行累加操作要慢。</p>
<pre><code>   造成这个情况因为线程有创建和上下文切换的开销。

   解决这个情况的方法有
    1.无锁并发编程：比如通过数据的ID按照Hash算法分段，不同的线程处理不同的数据，来避免使用锁。
    2.CAS算法
    3.协程：在单线程中实现多任务的调度。在单线程中维持多个任务间的切换
</code></pre>
<p>2.死锁：</p>
<pre><code>    经典的例子：例如1线程运行在加A锁的方法中调用加B锁的方法，2线程运行在加B锁的方法中调用加A锁的方法，这会引起两个线程互相等待对方释放锁。
	
    死锁的核心问题：但在现实中这样的代码不常见，更为常见的情况是1线程拿到锁后因为一些异常（死循环）没有释放掉锁。此时也会引起死锁。归根结底是对锁释放的错误。
	
    避免死锁的方法：
        1.避免一个线程同时获取多个锁
        2.一锁一资源，一一对应。尽量不要一对多和多对一。
        3.使用定时锁来替代实体锁机制。
        4.数据库锁中加锁和解锁必须在一个数据库连接里。</code></pre>
]]></content>
    </entry>
</feed>
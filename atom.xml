<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://wkf1115.github.io</id>
    <title>WKF1115</title>
    <updated>2020-04-24T10:11:18.572Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://wkf1115.github.io"/>
    <link rel="self" href="https://wkf1115.github.io/atom.xml"/>
    <subtitle>人若是看透自己，就不会小看别人。</subtitle>
    <logo>https://wkf1115.github.io/images/avatar.png</logo>
    <icon>https://wkf1115.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, WKF1115</rights>
    <entry>
        <title type="html"><![CDATA[深入了解Java虚拟机第二章笔记📒]]></title>
        <id>https://wkf1115.github.io/post/shen-ru-liao-jie-java-xu-ni-ji-di-er-zhang-bi-ji/</id>
        <link href="https://wkf1115.github.io/post/shen-ru-liao-jie-java-xu-ni-ji-di-er-zhang-bi-ji/">
        </link>
        <updated>2020-04-24T09:15:14.000Z</updated>
        <content type="html"><![CDATA[<p>第二章：垃圾收集器与内存分配策略</p>
<p>   首先，程序计数器，虚拟机栈和本地方法栈由于是编译时就可以确定内存大小的，所以不需要GC进行处理。 相反，Java堆和方法区不一样，只有在运行期才可以看出来需要创建多少对象，哪些对象不同的程序分支会分配不同数量的内存。<br>
   所以，需要GC进行处理，不然内存会有溢出或者泄漏的风险</p>
<ul>
<li>
<p>1.判断对象是否死去：</p>
<ul>
<li>
<p>1.引用计数算法：</p>
<p>给对象添加一个引用计数器，每当有地方引用它时，计数器+1，失去引用时，计数器-1，计数器为0的对象会被判断为失效。<br>
（主流的JVM没有使用此方法，因为它无法解决对象之间的循环引用。如果A对象的字段引用B对象，B对象的字段引用A对象，这两个对象已经失效，因为除了他们自己，外界已经无法访问。但如果使用引用计数算法，这两个对象不能被回收。）</p>
</li>
<li>
<p>2.可达性分析算法：</p>
<p>🍉主流的商用语言的主流实现都是这么判断的，通过一个GC Roots对象根节点向下搜索，当一个对象无法从GC Root向下搜索查到，证明这个对象不可用。如图中的5，6，7虽然相互引用，但是无法与根节点产生联系。<br>
<img src="https://wkf1115.github.io/post-images/1587720275460.png" alt="" loading="lazy"></p>
<ul>
<li>java中可作为GC Roots的对象包括：
<ul>
<li>1.虚拟机栈中引用的对象</li>
<li>2.方法区中类静态属性引用的对象</li>
<li>3.方法区中常量引用的对象</li>
<li>4.本地方法栈中Native方法引用的对象</li>
</ul>
</li>
</ul>
</li>
<li>
<p>3.引用的分类：</p>
<ul>
<li>
<p>谈到对象是否应该被销毁，就要谈到对象的引用。<br>
JDK1.2以前，如果引用类型保存着一个对象的内存地址就称为代表一个引用。<br>
🍉（这种分类方式不够好，因为这时一个对象只有存在引用和不存在引用两种情况）<br>
🍉因为很多时候，对象在内存空间足够的时候最好保留，内存没地方再按照对象优先级进行销毁。</p>
</li>
<li>
<p>1.强引用：就是一般的将对象指向一个引用Object obj = new Object();<br>
只要obj这个引用存在，GC就不会回收那个对象。</p>
</li>
<li>
<p>2.软引用：有用但非必须，内存即将溢出的时候GC会对它们进行回收</p>
</li>
<li>
<p>3.弱引用：比软引用更没用的对象，它们只能存活到下次GC发生之前。</p>
</li>
<li>
<p>4虚引用（幽灵引用，幻影引用）：最弱的引用关系，这个“虚”字并不会影响他的生存时间，仅仅是它被回收的时候可以收到一个系统通知。</p>
</li>
</ul>
</li>
<li>
<p>4.对象什么时候才叫真的死了？</p>
<ul>
<li>
<p>一个对象真的死亡，要经历两次标记过程。</p>
<ul>
<li>第一次：可达性分析发现一个对象没有和GC Roots相连接，会被第一次标记，并判断是否应该执行finalize()方法，如果没有必要执行，对象会被回收，如果有必要执行，进入下一步。</li>
<li>第二次：对象进入F-queue，该队列由Finalizer线程执行，JVM会触发对象的finalize()方法，并在触发一段时间后，不管有没有执行完成，都停止它。<br>
GC会在对象执行完finalize()方法后查看有没有和其他引用相连接，如果建立连接，那么GC会停止回收它。这就是第二次标记。如果没有建立连接，那么难逃回收的命运了。</li>
</ul>
<p>🍉值得注意的是，一个对象系统只会自动调用一次finalize()，如果执行过一次，第二次就不会自动执行了。</p>
<p>🍎这个方法毕竟不是C/C++的析构函数，它被设计出来本身就是一种妥协，这个方法代价高昂，而且十分不确定顺序（由于F-queue是新开的一个线程，所以可能导致并发执行的顺序问题），不如使用try-catch-finally，作者建议大家不要使用它。</p>
</li>
</ul>
</li>
<li>
<p>5.方法区的GC回收：</p>
<ul>
<li>主要是回收废弃常量和无用的类，例如常量池中存在的数据，没有任何对象等于这个数据，下次GC的时候这个常量会被回收。这样就是判断废弃常量的条件。</li>
<li>那么废弃的类如何判断呢，有三个条件：
<ul>
<li>1.该类所有实例已被回收</li>
<li>2.该类所有的ClassLoader都被回收</li>
<li>3.无法通过反射访问该类的方法。<br>
虚拟机可以对满足这些条件的类进行回收，仅仅是可以，而不是一定会。</li>
<li>🍉大量使用反射，动态代理，jsp等方式要进行类回收，不然永久代可能会溢出。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>2.垃圾收集算法：</p>
<ul>
<li>
<p>1.标记-清除算法：</p>
<ul>
<li>最基础的收集算法，先标记，再直接清除。标记部分上面有说过。</li>
<li>这个算法的不足就是清除后可能剩下很多小空间，没有足够的空间导致分配不了大对象，只能再触发一次GC。<br>
￼<img src="https://wkf1115.github.io/post-images/1587720959740.png" alt="" loading="lazy"></li>
</ul>
</li>
<li>
<p>2.复制算法：</p>
<ul>
<li>为了解决上面方法的性能问题，大多数商业虚拟机都使用复制方法。</li>
<li>这种算法是先将内存划分为两块，每次使用的时候使用其中的一块，当这块内存满了之后，触发GC清理之后将剩余可用对象复制到另外一块上，清理出之前这块内存区域。<br>
这样可以使空间比较连续，分配起来很方便。<br>
￼<img src="https://wkf1115.github.io/post-images/1587720987444.png" alt="" loading="lazy"></li>
<li>🍉研究表明，98%的对象生命周期极短，需要被销毁，所以内存空间不需要按照上面的1:1分配。实际上分配方式是8:1，有一个较大的Eden空间和两个小的Survivor空间，每次使用一个Survivor空间和Eden空间。发生复制的时候将所有存活对象存在另外一块Survivor空间。</li>
<li>🍉如果发生Survivor空间不足的情况，这个Survivor空间会向JVM申请内存担保，也就是借一些内存来使用。多余出来的对象会直接进入老年代。</li>
</ul>
</li>
<li>
<p>3.标记-整理算法：</p>
<ul>
<li>由于第二种方法在对象存活率较高的时候需要很多复制操作，而且需要很多担保空间。所以需要这种算法</li>
<li>这种方法可以说是第一种方法的优化，区别在于标记后将存活的整理到一侧，销毁的进行删除。<br>
￼<img src="https://wkf1115.github.io/post-images/1587721017366.png" alt="" loading="lazy"></li>
</ul>
</li>
<li>
<p>4.分代收集：</p>
<ul>
<li>
<p>这种方法可以说和上面的搭配使用，它根据对象的存活周期将Java堆划分为新生代和老年代。<br>
（新生代中对象死亡率高，老年代中对象存活率高）</p>
</li>
<li>
<p>🍉在新生代中使用复制算法，在老年代中使用标记-整理算法。</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>3.HotSpot算法实现：</p>
<ul>
<li>
<p>1.枚举根节点：</p>
<ul>
<li>这个算法是可达性分析，由于可达性分析找的根节点（GC Roots）分布在全局性的引用（常量和静态类属性）和执行上下文中，逐个检查开销很大。</li>
<li>所以在JVM中有一组叫做OopMap的数据结构来计算引用位置，在类加载完成后OopMap就能计算引用。GC在暂停所有线程进行扫描的时候直接扫描OopMap就可以判断哪些是该回收的对象。</li>
</ul>
</li>
<li>
<p>2.安全点：</p>
<ul>
<li>
<p>上一点的GC根据OopMap进行回收，但是OopMap在程序执行的时候需要动态生成，因为对象或引用会变化，那么如何选取生成方式呢？很显然，OopMap不能每一句都生成一次。</p>
</li>
<li>
<p>🍉安全点（Safepoint）就是生成OopMap的位置，线程只有在安全点才能进行暂停，等待GC的回收。</p>
</li>
<li>
<p>🍉安全点一般在方法调用，循环跳转，异常跳转这种功能指令生成。</p>
</li>
<li>
<p>🍎抢先式中断和主动式中断：</p>
<ul>
<li>1.由于线程必须执行到安全点才能进行终止，所以抢先式中断就是在GC发生时将所有线程中断，如果哪个线程没有到达安全点，让它启动，继续跑到安全点再停止。但是现在几乎没有虚拟机实现这种中断。</li>
<li>2.主动式中断就是各个线程在运行到安全点时，检查此时GC是否在工作，如果在GC要执行了就执行中断。GC在启动时会设置一个标志为真。其他线程读取这个标志就能实现交互。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>3.安全区域（Safe Region）：</p>
<ul>
<li>安全点只是保证了在程序执行的时候，很快会进入中断。但如果程序休眠或者中断的时候，安全点就无法保证它的继续中断了。因为线程休眠或中断时并没有为其分配CPU时间，也就无法响应GC。</li>
<li>🍉安全区域可以看作安全点的扩展，在线程进入安全区域代码的时候和安全点一样，此时GC可以执行。如果GC还没有结束，安全区域内的线程要执行结束，该线程会检查GC过程是否结束，如果结束，它会继续执行，否则等待离开信号。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>4.七大垃圾收集器：<br>
如果说收集算法是内存回收的方法，那么垃圾收集器就是内存回收的具体实现。<br>
￼<img src="https://wkf1115.github.io/post-images/1587721144014.png" alt="" loading="lazy"><br>
上面是新生代区域，下面是老年代区域。</p>
<ul>
<li>
<p>1.Serial收集器：最基本，发展最悠久的收集器</p>
<ul>
<li>这是一个单线程收集器，十分传统，新生代的时候使用复制算法，老年代使用标记-整理算法，在这个收集器运行的时候其他用户线程必须被停止。它的停止线程操作对于用户体验非常差，因为会有频繁的卡顿。</li>
<li>🍉但它也不是一无是处，毕竟简单又高效，停止其他线程，这使得它几乎没有用于线程交互的开销，可以专心进行收集。在新生代中可以使用。</li>
<li>到现在为止Client模式下默认的新生代收集器还是Serial<br>
￼<img src="https://wkf1115.github.io/post-images/1587721176137.png" alt="" loading="lazy"></li>
</ul>
</li>
<li>
<p>2.ParNew收集器：多线程的Serial收集器</p>
<ul>
<li>该线程就是Serial收集器的多线程版本，没有什么创新，但它是Server模式下首选的新生代收集器，因为他可以和CMS收集器进行合作，一个新生代，一个老年代。</li>
<li>当然，在但CPU的情况下ParNew是不如Serial，因为有线程间的开销，但是一旦CPU数量增加，ParNew就大大的超过Serial。<br>
￼<img src="https://wkf1115.github.io/post-images/1587721202771.png" alt="" loading="lazy"></li>
</ul>
</li>
<li>
<p>3.Parallel Scavenge收集器：</p>
<ul>
<li>这也是一个新生代收集器，也是采用复制算法，并发收集器。但它的重点和其他收集器都不同，其它收集器都致力于减小用户线程的停顿时间，而这个收集器致力于控制吞吐量。（吞吐量=运行用户代码时间/运行用户代码时间+垃圾收集时间）</li>
<li>🍉也就是说可以集中控制垃圾收集时间，适用于后台不需要太多的用户交互时使用。</li>
<li>🍉有三个参数可以修改，一个是控制最大垃圾收集停顿时间，一个是直接设置吞吐量大小，还有一个是开关，开启后GC可以自动根据性能监控动态指定参数，控制吞吐量。</li>
</ul>
</li>
<li>
<p>4.Serial Old收集器：</p>
<ul>
<li>Serial收集器的老年代版本，在老年代上采取标记整理算法。主要用于Client模式下的JVM</li>
</ul>
</li>
<li>
<p>5.Parallel Old收集器：</p>
<ul>
<li>Parallel Scavenge收集器的老年代版本，使用多线程和标记-整理算法。它和Parallel Scavenge收集器搭配可以用于“吞吐量优先的情况”。</li>
</ul>
</li>
<li>
<p>6.CMS收集器：</p>
<ul>
<li>
<p>这个收集器致力于获取最短回收停顿时间，并发收集，减小响应延迟，所以它主要用于服务端上，能够给用户带来较好的体验。</p>
</li>
<li>
<p>🍉基于标记—清除。分为四个步骤：<br>
1.初始标记<br>
2.并发标记<br>
3.重新标记<br>
4.并发清除</p>
<ul>
<li>
<p>首先，GC先对GC Roots关联的对象进行一次简单的标记，速度很快，这步骤是初始标记。</p>
</li>
<li>
<p>之后进入并发标记，这段时间内GC进行详细的查找应该被销毁的对象。</p>
</li>
<li>
<p>接下来是重新标记，这段时间GC会整理并发标记期间，用户同时运行产生的对象变动。</p>
</li>
<li>
<p>并发清除就是清除掉销毁对象。</p>
</li>
<li>
<p>🍎初始标记和重新标记是不并发的，这两个过程中其他线程会被暂停，但是CMS将占用时间最多的操作都放在了并发标记和并发清除中，这两个时间内所有线程同时运行，可以大大减少用户的停顿时间。</p>
</li>
<li>
<p>🍉它也有3个明显的缺点：</p>
<ul>
<li>
<p>1.并发阶段虽然不会停顿其他线程，但是由于对CPU资源的占用，应用程序会变慢，总吞吐量降低。<br>
（如果用户此时cpu资源不太足，GC和用户线程并发的执行会严重缩减用户剩余的CPU资源，导致严重卡顿）。</p>
</li>
<li>
<p>2.无法处理浮动垃圾，在并发阶段时用户还会产生新的垃圾，这部分垃圾没有被标记，只能等待下次GC回收。然而这一部分垃圾也需要占用空间，这就导致了GC不能等老年代内存满了之后再收集，需要预留一些额外空间。如果浮动垃圾溢出，JVM会采用Serial Old来临时处理老年代空间，这就导致大量的开销。</p>
</li>
<li>
<p>3.CMS收集的算法采用的是标记—整理算法，上面也讲过，这会导致大量内存碎片空间，不能存储大型的对象。为了解决这个问题，CMS采用在顶不住碎片空间带来的压力时使用内存碎片整理，但是整理过程其它用户线程必须被暂停，整理过程无法并发。</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>7.G1收集器：</p>
<ul>
<li>
<p>首先，G1收集器有如下特点：</p>
<ul>
<li>1.回收期间可以使用多CPU通过并发让其他线程依旧运行。</li>
<li>2.分代收集中的新生代和老年代不再物理隔离，而是概念上的隔离。</li>
<li>🍉（比CMS好的地方）3.基于标记—整理算法，相比较于标记—清理算法有着更多优势。</li>
<li>🍉（比CMS好的地方）4.可建立预测停顿时间模型，可以指定回收操作不超过多少毫秒，可以进行规划。<br>
🍉上面也提到了，G1不再物理隔离新生代和老年代，它将堆划分成了一个个Region，每次根据Region内垃圾价值的大小来选择回收哪个Region。在后台维护一个优先列表储存回收顺序。</li>
</ul>
<p>🍎思考问题：化整为零的思想看起来很简单，但是G1是如何保证扫描GC Roots可以分开扫描的？如果整体扫描那么开销变化并不大。（其实上面的划分新生代老年代的收集器也会有这个问题，回收新生代也要扫描老年代，增加开销）。<br>
🍎解决方案：G1每一个Region有一个Remembered Set，在JVM发现程序对引用类型属性进行写操作时，会在写操作前加上一个写屏障，判断引用类型是否处于不同的Region中（这是G1的分Region情况，在新生代和老年代情况就是检查新老年代对象是否引用了新生代对象），如果是，就把对象信息引用记录添加到其它Region的Remembere Set中。这样在不全堆扫描就内存回收的时候就不会遗漏某些对象了。</p>
<p>🍉详细过程：初始标记—&gt;并发标记—&gt;最终标记—&gt;筛选回收</p>
<ul>
<li>G1的初始标记和并发标记过程和CMS相似。</li>
<li>最终标记是使用Remembered Set Logs将并发过程中程序变动的对象记录保存在Remembered Set中，这段时间也是要停止用户线程。</li>
<li>筛选回收过程就是对Remembered Set进行排序。选出回收效率最高的Region。这个过程是可以和用户线程并发的，但效果并不理想。因为只回收一部分Region，回收时间用户可控，所以不如暂停用户线程来提高收集效率。<br>
￼<img src="https://wkf1115.github.io/post-images/1587721238844.png" alt="" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>5.如何去读GC日志：<br>
<img src="https://wkf1115.github.io/post-images/1587721270465.png" alt="" loading="lazy"></p>
<ul>
<li>一些术语：
<ul>
<li>1.GC和Full GC：前面加上Full代表此次GC的停顿类型，是否发生了停止其他线程的（stop-the-world）操作。如果调用System.gc(),那么会显示Full GC(System)。Full GC也代表着老年代GC。</li>
<li>2.DefNew，Tenured，Perm表示GC发生的区域，DefNew代表Default New Generation。可以看出这些区域是由使用的不同收集器和不同代决定的。</li>
<li>3.3324k—&gt;152K(3712K)，GC前该内存区域已使用空间—&gt;GC后该内存已使用空间(该内存区域总空间)，后面的3324K—&gt;152K(11904K)代表，GC前Java堆已使用空间—&gt;GC后Java堆已使用空间(java堆总空间)</li>
<li>4.后面的时间代表GC消耗时间。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>6.内存分配策略：</p>
<ul>
<li>
<p>对象内存分配主要是在新生代的Eden区上分配，如果启动了TLAB，会在TLAB上按照线程优先分配，少数情况下会分配在老年代中。</p>
</li>
<li>
<p>🍉大多数情况下，对象在新生代Eden区中分配，Eden没空间的时候，虚拟机会发起新生代GC（Minor GC）。如果Minor GC还不够空间，会触发分配担保机制，扩大老年代空间并将对象都装入老年区中。</p>
</li>
<li>
<p>🍉大对象直接进入老年代，需要大量连续内存空间来储存的对象会直接进入老年代。</p>
</li>
<li>
<p>🍉长期存活的对象将进入老年代：虚拟机为每个对象都定义了年龄，如果对象能存活到Survivor区域中，并在里面存活一段时间（默认为15岁），那么它就会进入老年代中。<br>
注：对象年龄的判断是动态的，如果Survivor空间中同年龄的对象占到了Survivor总空间一半以上，那么这个年龄会被设置成阈值，高于此年龄的对象会被保送到老年代。</p>
</li>
<li>
<p>🍉空间分配担保也不是每次都能够成功的，在Survivor空间满了之后，GC会参考历代老年代对象平均对象容量大小，通过这个来判断是否进行Full GC来清理老年代。如果判断失误，强行担保，那么GC在失败后会立刻进行Full GC。</p>
</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[深入了解Java虚拟机第一章笔记📒]]></title>
        <id>https://wkf1115.github.io/post/shen-ru-liao-jie-java-xu-ni-ji-di-yi-zhang-bi-ji/</id>
        <link href="https://wkf1115.github.io/post/shen-ru-liao-jie-java-xu-ni-ji-di-yi-zhang-bi-ji/">
        </link>
        <updated>2020-04-21T11:58:33.000Z</updated>
        <content type="html"><![CDATA[<p>第一章：Java内存区域与内存溢出异常</p>
<p>1.运行时数据区域：</p>
<p>java虚拟机在执行过程中会将内存分为若干数据区域，每个区域都有用途，创建销毁时间，有些是JVM管理，有些是用户管理。</p>
<ul>
<li>
<p>1.程序计数器：</p>
<ul>
<li>
<p>🍉是一块较小的内存空间，作用就是指使JVM下一条指令应该是哪条（仅仅是概念模型中，实际JVM可能通过更高效的方式来实现）</p>
</li>
<li>
<p>🍎由于JVM的多线程通过线程频繁切换进行程序执行，同一时间一个处理器只会执行一条线程，为了切换后可以正确接着执行，所以每个线程都有一个独立的程序计数器（线程私有的内存）</p>
</li>
<li>
<p>🍉如果线程执行一个Java方法，计数器记录正在执行的字节码指令地址，如果是native方法，计数器为空（虽然为空，也没有OutOfMemoryError）</p>
</li>
</ul>
</li>
<li>
<p>2.虚拟机栈：</p>
<ul>
<li>
<p>🍎它也是线程私有的，和线程生命周期相同（描述Java方法执行的内存模型）。</p>
</li>
<li>
<p>🍉每个方法执行时都会创建一个栈帧来代表这个方法，栈帧的入和出虚拟机栈代表了方法的调用开始和执行结束。<br>
（栈帧用于储存局部变量表，操作数栈，动态链接，方法出口）</p>
<p>🍌大家对JVM简单划分的堆和栈，这个栈实际上可能大部分时候指的是虚拟机栈中的🍌局部变量表🍌。</p>
</li>
<li>
<p>🍑局部变量表：</p>
<ul>
<li>1.局部变量表存放了存在于编译期的所有基本数据类型，对象引用（指针类型），returnAddress类型（指向字节码指令的地址）。</li>
<li>2.64位长度的变量占用两个变量空间。其余占用一个。</li>
<li>3.局部变量表的内存空间在编译时就分配完成，在程序使用时局部变量的空间大小是完全确定的，方法运行不会改变它的大小。</li>
<li>4.会出现两种异常：StackOverflowError，线程请求的栈深度大于虚拟机允许的深度。<br>
OutOfMemoryError，虚拟机栈动态扩展时申请不到足够内存。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>3.本地方法栈：</p>
<ul>
<li>它的作用和虚拟机栈作用十分相似，虚拟机栈执行java代码，本地方法栈执行native方法。基本与虚拟机栈相同，虚拟机栈可以自由实现本地方法栈。有些虚拟机将本地方法栈和虚拟机栈合在一起用。</li>
</ul>
</li>
<li>
<p>4.Java堆：这是虚拟机内存中最大的一块。</p>
<ul>
<li>
<p>1.它被所有线程共享，在虚拟机启动时自动创建。</p>
</li>
<li>
<p>2.作用几乎就是存放对象。（但是随着JIT编译器的发展与逃逸技术分析，栈上分配，标量替换导致了可能不是所有的对象都在堆上。）</p>
</li>
<li>
<p>3.它是垃圾收集器的主要管理区域，也被叫做GC堆（我觉得叫垃圾堆也不错）</p>
</li>
<li>
<p>4.可以细分为很多区域，不过不管如何划分，堆中储存的对象都不会变化。后续的章节会详细介绍。</p>
</li>
<li>
<p>5.根据规范规定，堆可以在物理内存上不连续，只要在逻辑上连续就可以。</p>
</li>
<li>
<p>6.和虚拟机栈一样，如果扩展时内存不足，会报OutOfMemoryError异常。</p>
<ul>
<li>
<p>🍑.方法区：堆的一部分</p>
<ul>
<li>
<p>1.作用就是储存类信息，常量，静态变量这种编译期就被加载的值。</p>
</li>
<li>
<p>2.它和堆一样，被所有线程共享。</p>
<p>🍉java虚拟机规范将其描述为堆的一个逻辑部分，换句话说，它是堆的一部分。</p>
</li>
<li>
<p>3.这部分的区域不是很容易被GC回收，虽然GC也掌管这片区域。</p>
</li>
<li>
<p>🍑.运行时常量池：方法区的一部分</p>
<ul>
<li>存放编译期生成的各种字面量和符号引用。它相比较于Class文件常量池的另外一个特征就是具备动态性。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>5.直接内存：</p>
<ul>
<li>它并不是JVM内存中的一部分，有时候用于Native堆和Java堆的数据交互，将Native函数分配在直接内存中。<br>
（它的分配不会受到Java堆大小的限制）</li>
</ul>
</li>
</ul>
<p>2.Hostspot虚拟机探秘：</p>
<ul>
<li>
<p>1.对象的创建：</p>
<ul>
<li>
<p>1.虚拟机遇到new指令  ——&gt;  检查这个new的类(指令参数)能否在常量池中 ——&gt;  判断这个类是否已经加载解析和初始化过 ———&gt;  如果没有，必须先执行类加载    ——&gt;  通过后，为对象分配内存。到这里，详细讲解一下分配内存</p>
<ul>
<li>
<p>🍉首先，分配内存大小在类加载结束后可以确定，分配内存等同于从java堆中分出一块确定大小的内存。这样就有两种情况：</p>
<ul>
<li>
<p>（指针碰撞）1.如果内存是绝对规整的（连续的）那么会有一个指针用来放在内存中分界点上。分配内存就是把指针向空闲区域挪动一定大小的过程。</p>
</li>
<li>
<p>（空闲列表）2.如果内存不连续，那么JVM会维护一个列表，列表记录哪些内存区域可用，分配时列表挑选一块足够大的区域。</p>
</li>
</ul>
</li>
<li>
<p>🍉选择哪种分配方式由Java堆是否规整决定，也就是由垃圾收集器进行选择。</p>
</li>
<li>
<p>🍉创建对象的并发问题：</p>
<ul>
<li>由于开辟对象空间和修改指针有多步操作，所以一定会有并发问题。</li>
<li>解决方式就是每个线程在JAVA堆中预先分配一小块内存，这个内存叫做本地线程分配缓冲（TLAB）。哪个线程要分配内存，就在哪个线程的TLAB上进行分配。只有多个TLAB之间的切换，再用CAS加锁的方式进行处理。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>2.内存分配结束 ——&gt;  该空间会被初始化成0值（如果使用了TLAB，那么在TLAB分配阶段就会进行）</p>
<ul>
<li>🍉这个初始化行为是为了保证该对象的字段不用初始化处值就可以直接使用<br>
（这就是为什么类变量可以直接用，而方法内变量必须要手动指定初值）</li>
</ul>
</li>
<li>
<p>3.空间初始化结束 ——&gt; JVM写入对象头（比如对象是哪个类的实例，元数据信息，hashcode，GC分代年龄，偏向锁等）</p>
<ul>
<li>🍎到这里，从JVM的角度来讲，一个新对象产生了，但是从Java程序的角度看，并没有结束，因为init方法没有执行（构造方法）<br>
所以一般来说new指令执行结束后会执行init方法来初始化对象。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>2.对象的内存布局：分为三部分</p>
<ul>
<li>
<p>1.对象头（header）包含两个部分：储存对象自身的运行时数据（Mark Word）和类型指针</p>
<ul>
<li>
<p>🍉储存对象自身的运行时数据（Mark Word）：哈希码，GC分代年龄，锁状态标志，线程持有的锁，偏向线程ID，偏向时间戳。这个数据长度分别为32bit和64bit，虽然长度固定，但是由于需要在尽可能小的地方储存信息，所以它的结构不确定，会被JVM进行调整。</p>
</li>
<li>
<p>🍉类型指针，指向它类元数据的指针，这个可以确定该对象是哪个类的实例。<br>
（如果是一个Java数组，那么对象头中必须有一块用于记录数组长度的数据，因为数组元数据无法确定数组大小）</p>
</li>
</ul>
</li>
<li>
<p>2.实例数据：</p>
<ul>
<li>对象真正储存的有效信息，是各种类型的字段内容（储存顺序受JVM分配策略参数影响）<br>
🍉大体顺序是先写入父类属性，再写入子类属性，有些子类较窄的变量可能插入到父类变量之间</li>
</ul>
</li>
<li>
<p>3.对齐填充：</p>
<ul>
<li>占位作用，HotSpot VM要求对象大小必须是8字节的整数倍，所以需要对齐填充来占位。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>3.对象的访问定位：</p>
<ul>
<li>
<p>对象创建后，运行程序时需要通过使用栈上的引用类型数据来操作对象。</p>
</li>
<li>
<p>而引用类型只是一个指向对象的引用，并没有定义如何去访问对象，所以访问方式也是通过不同虚拟机的不同实现。</p>
</li>
<li>
<p>主流访问方式有句柄和直接指针两种。</p>
<ul>
<li>
<p>1.句柄访问：<br>
Java堆中划分一片区域作为句柄池，引用类型指向句柄池中的句柄，每一个句柄都包含一个对象实例和对象类型（放在方法区，代表这个对象由哪个类实例化）。<br>
<img src="https://wkf1115.github.io/post-images/1587470457325.png" alt="" loading="lazy"><br>
￼</p>
</li>
<li>
<p>2.指针访问：<br>
引用直接指向对象的地址，对象实例中保存指向对象类型的指针。<br>
<img src="https://wkf1115.github.io/post-images/1587470476290.png" alt="" loading="lazy"><br>
￼</p>
<p>🍉句柄访问虽然速度慢，但稳定，因为对象被移动时只有句柄中的实例数据指针会被修改，引用本身不需要修改。而直接指针访问速度更快。</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>4.实际情况栈的内存溢出（OutOfMemoryError）和StackOverflowError异常：</p>
<ul>
<li>
<p>规范中定义这两个异常，前者为线程请求的栈深度大于JVM所允许的最大深度，后者为虚拟机扩展栈时无法申请到足够的内存空间。</p>
</li>
<li>
<p>在描述上，这两个异常看起来差不多，都是内存不够。<br>
实际上，在单线程的情况下，基本只能发生后者异常。无论是栈帧太大，还是内存不足。<br>
（JVM默认单线程的栈是可以无限扩展的，不会内存溢出）<br>
在开很多线程的时候，可能发生前者异常，线程过多，栈帧过多，导致内存溢出。</p>
<ul>
<li>🍎多线程情况下内存溢出，可以在不影响程序占用栈帧的情况下，缩减栈容量和减少堆内存来解决。<br>
内存缩减反而不溢出了，因为缩减的是单个线程的空栈帧。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>5.方法区和运行时常量池溢出：<br>
动态生成大量Class时，大量JSP（初次运行会编译为Java类）或JSP动态生成JSP文件，方法区会溢出。<br>
常量池内常量设置过多时常量池会溢出。</p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java并发编程艺术第四章学习笔记📒]]></title>
        <id>https://wkf1115.github.io/post/java-bing-fa-bian-cheng-yi-zhu-di-si-zhang-xue-xi-bi-ji/</id>
        <link href="https://wkf1115.github.io/post/java-bing-fa-bian-cheng-yi-zhu-di-si-zhang-xue-xi-bi-ji/">
        </link>
        <updated>2020-04-21T09:06:58.000Z</updated>
        <content type="html"><![CDATA[<p>第四章：JAVA并发编程基础</p>
<pre><code>实际上JAVA天生是一个多线程语言，在运行mian方法的时候就会执行多个线程。多线程带来了许多好处，更多的处理器核心，更快的响应时间，更好的编程模型。
</code></pre>
<p>1.线程优先级：</p>
<pre><code>    现代操作系统分配内存时会分出一个个时间片，每一个线程使用不等数量的时间片，线程时间片用完了就会发生线程调度。

    线程优先级就是线程需要多少处理器的资源。

    首先Java线程是通过一个名字叫做priority的整形成员变量来控制优先级，优先级的范围是1～10。
    可以通过setPriority(int)方法设置优先级，默认优先级是5，优先级高的线程时间片要多于低的。

    🍉频繁阻塞（休眠或者I/O操作）的线程需要设置较高的优先级，偏重计算（需要较多cpu时间或者偏运算）的线程需要较低的优先级。
    🍉这样可以避免cpu资源被偏重计算的线程独占。
    
    🍎线程优先性的设置，在很多实际jvm环境中都不会生效，所以这条仅供参考。
</code></pre>
<p>2.线程的状态：</p>
<pre><code>    线程在生命周期中有6种状态：
        
        1.NEW：初始状态，线程刚刚被构建，还没有调用start()方法。
        
        2.RUNNABLE：运行状态，操作系统的就绪和运行两种状态都被称作运行状态。
        
        3.BLOCKED：阻塞状态，表示被锁阻塞。

        4.WAITING：等待状态，表示该线程需要其它线程作出一些特定动作。
        
        5.TIME_WAITING：超时等待，不同于WAITING，他可以在指定时间内自行返回。

        6.TERMINATED：终止状态，线程执行完毕。
    
    线程的运行流程：线程创建后会调用start()方法开始运行，线程执行wait()方法后，线程会进入等待状态。等待状态需要其他线程的通知才能变回运行状态，
                    超时等待时间到达将会返回到运行状态，线程在没有获取锁的时候会进入阻塞状态，线程在执行Runnable的run()方法后会进入终止状态。
</code></pre>
<p>3.Daemon线程：</p>
<pre><code>        🍉Daemon线程主要用于支持程序后台调度和支持性工作，当JVM中不存在非Daemon线程的时候，JVM会退出。
</code></pre>
<p>4.线程的创建：</p>
<pre><code>        一个新线程的构造是由其parent线程进行空间分配的，它继承了parent线程是否为Daemon，优先级，加载资源的contextClassLoader还有可继承的ThreadLocal，
        这个线程还会有唯一的ID。
        
        创建结束之后调用start()方法就可以启动线程，只要线程规划器空闲，应该立即启动该线程。
</code></pre>
<p>5.线程的终止：</p>
<pre><code>        线程的中断操作可以作为一种简便的线程间交互方式，同时也可在线程内设置变量来控制线程的终止。
</code></pre>
<p>6.synchronized的方式：</p>
<pre><code>        任意线程对Object的访问需要经过它的Monitor（监视器），线程首先对监视器执行Monitor.Enter()方法，如果Enter失败，线程会进入一个同步队列。
        线程在释放锁的时候，会执行Monitor.exit()方法，同步队列检测到exit方法被调用时，队头的线程会执行出队操作并执行Monitor.Enter()方法。
</code></pre>
<p>7.wait()和notify()：</p>
<pre><code>        1.wait()和notify(),notifyAll()在调用时需要给先获取到锁。
        
        2.调用wait()方法后，线程状态变为waiting，并加入到同步队列中。
    
        3.notify调用后，线程不会立即从wait返回，需要调用notify的线程释放锁后，wait线程获取到锁，才有可能返回。
            🍉（notify方法将一个wait线程从等待队列中移到同步队列中，notifyAll将等待队列中全部的线程都移到同步队列，被移动的线程由waiting变为blocked状态）
￼
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://wkf1115.github.io/post-images/1587460118713.png" alt="" loading="lazy"></figure>
<p>8.等待/通知的经典范式：</p>
<pre><code>        等待方：
        
            1.获取对象的锁
            2.如果条件不满足，调用对象的wait()方法，如果条件满足停止wait()
        
        通知方：
            
            1.获得对象的锁
            2.改变条件
            3.通知所有等待的线程
</code></pre>
<p>9.thread.join()的使用：</p>
<pre><code>        如果一个线程A执行了B.join()语句，那么A等待B线程结束之后才会从B.join()返回（继续执行）。
        从源码角度来看，join()的实现使用了等待/通知的经典范式—加锁，循环，和处理逻辑，只不过条件变为了线程是否活着。
</code></pre>
<p>10.ThreadLocal的使用：</p>
<pre><code>        ThreadLocal是线程变量，是以ThreadLocal对象为键，任意对象为值的储存结构。这个结构是附带在线程上的。也就是一个线程可以根据一个ThreadLocal对象查询				该线程的某些信息。

        springAOP中可以用到此类方法。因为这个变量可以将方法拆分开，中间插入其他方法，通过变量来传递信息。</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java并发编程艺术第三章学习笔记📒]]></title>
        <id>https://wkf1115.github.io/post/java-bing-fa-bian-cheng-yi-zhu-di-san-zhang-xue-xi-bi-ji/</id>
        <link href="https://wkf1115.github.io/post/java-bing-fa-bian-cheng-yi-zhu-di-san-zhang-xue-xi-bi-ji/">
        </link>
        <updated>2020-04-21T09:02:00.000Z</updated>
        <content type="html"><![CDATA[<p>第三章：java内存模型（JMM）</p>
<ul>
<li>
<p>为了了解内存模型的基础，我们就要明白线程拥有两种通信机制：共享内存和消息传递</p>
</li>
<li>
<p>共享内存通过主程序的内存来实现线程间的通信，所以它是隐式通信，同时它需要程序员显式的制定的执行顺序，也叫显式同步。</p>
</li>
<li>
<p>消息传递通过线程之间传递消息来实现线程间的通信，所以它是显式通信，同时它不需要程序员人为指定执行顺序，因为消息发送必须在消息接收前，也叫隐式同步。</p>
</li>
<li>
<p>1.JMM的抽象结构：</p>
<ul>
<li>
<p>java中的共享变量（实例域，静态域，数组元素）都储存在堆内存中，这部分内存是线程共享的，也就是说他们需要受到JMM的管理。</p>
</li>
<li>
<p>从抽象的角度看，每个线程拥有一个私有的本地内存，线程之间的共享变量都存在主内存中，三者之间的关系类似cpu与高速缓存与主内存的关系。</p>
<ul>
<li>🍉本地内存与主内存之间的关系通过JMM进行控制</li>
<li>🍉线程之间的通信就类似于上一章的cpu，高速缓存，主内存模型中cpu与cpu之间的通信。（必须经过主内存）</li>
<li>🍉当然，这仅仅为抽象结构，实际并不存在。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>2.🍊源代码和指令序列的重排序：（🍊本章重点，JMM离不开重排序）</p>
<ul>
<li>
<p>在执行程序时，我们写的源代码为了发挥最大性能，常常会被重排序。</p>
<ul>
<li>
<p>1.编译器重排序：</p>
<pre><code>      编译器在不影响单线程语义的情况下，会重新安排语句的执行顺序。
</code></pre>
</li>
<li>
<p>2.指令级并行的重排序：</p>
<pre><code>      cpu在机器指令不存在数据依赖性的情况，可以改变机器指令的执行顺序。
</code></pre>
</li>
<li>
<p>3.内存系统的重排序：</p>
<pre><code>      对cpu的读取和储存操作进行修改，修改读写顺序。
</code></pre>
</li>
</ul>
</li>
<li>
<p>🍉对以上的2，3操作，JMM会对出现问题的重排序进行监管，为了不让程序出现排序错误，会在生成指令序列时在序列中插入内存屏障，通过屏障来禁止重排序</p>
</li>
</ul>
</li>
<li>
<p>3.屏障类型：</p>
<ul>
<li>
<p>1.LoadLoad屏障，确保前面的装载（读取）命令先于后面的装载（读取）命令</p>
</li>
<li>
<p>2.StoreStore屏障，确保前面的存储（向主内存中刷入缓存区数据）先于后面的存储（向主内存中刷入缓存区数据）</p>
</li>
<li>
<p>3.LoadStore屏障，确保前面的装载先于后面的存储</p>
</li>
<li>
<p>4.StoreLoad屏障，这个屏障比较特殊，它会使前面的内存访问指令（存储和读取）先于后面的内存访问指令（存储和读取）</p>
</li>
</ul>
</li>
<li>
<p>4.happens-before：<br>
这是一个规则，代表着两个操作之间的关系。</p>
<ul>
<li>
<p>1.一个线程中的每个操作happens-before于该线程中的任意后续操作（程序顺序规则）</p>
</li>
<li>
<p>2.对于一个锁的解锁，happens-before于随后对这个锁的加锁，也就是先解锁再加锁（监视器锁规则）</p>
</li>
<li>
<p>3.对于一个volatile域的写，happens-before于任意后续对这个volatile域的读。（volatile变量规则）</p>
</li>
<li>
<p>🍎4.如果 A happens-before  B，B happens-before C，那么 A happens-before C （传递性）</p>
</li>
<li>
<p>5.如果线程A执行ThreadB.start()操作，那么A线程的ThreadB.start()操作happens-before线程B的任意操作（start（）规则）</p>
</li>
<li>
<p>6.如果线程A执行ThreadB.join()操作并成功返回，那么线程B的任意操作happens-beforeA线程的ThreadB.join()操作成功返回(join()规则)</p>
<p>那么我们定义的happens-before规则和JMM有什么关系呢。</p>
<p>🍉JMM在底层定义了多个cpu重排序规则，JMM进而通过规则禁止某个重排序（通过屏障），而这个屏障呈现出来就是一个一个的happens-before规则。</p>
</li>
</ul>
</li>
<li>
<p>5.数据依赖性：</p>
<ul>
<li>
<p>数据依赖性是什么呢，它是代表了两个操作如果被重排序了会影响结果。</p>
</li>
<li>
<p>如果两个操作使用了同一个变量，且这两个操作中有一个为写操作，那么这两个操作就有了数据依赖性。</p>
</li>
<li>
<p>例如写后读，写后写，读后写。这三个操作只要被重写，执行结果就会改变。</p>
</li>
<li>
<p>🍉在JMM排序时，会遵循数据依赖性规则，具有数据依赖的两个操作不会被重排序。要注意的是这仅仅是在单个线程中遵循的规则，线程之间并不遵守。</p>
</li>
</ul>
</li>
<li>
<p>6.as-if-serial:</p>
<ul>
<li>as-if-serial语义就是不管怎么重排序，编译器和cpu都不能对存在数据依赖性的操作作重排序，这样保证了程序执行的结果正确。</li>
<li>🍉它为程序员营造了一个幻觉：单线程程序是按照程序的顺序来执行的。</li>
<li>🍎程序遇到if条件，cpu会执行猜测操作，即先计算if方法体的内容，将结果保存到ROB中，如果if条件为真，再把计算的值赋给if方法体内的变量，这也是一种变相的重排序。在单线程内不会影响程序结果。但在并发情况下会出现结果错误。</li>
</ul>
</li>
<li>
<p>7.顺序一致性内存模型（被同步过的程序）：</p>
<ul>
<li>
<p>顺序一致性内存模型是被理想化的参考模型，程序员在实际操作的时候可以讲程序的执行顺序看作顺序一致性内存模型。</p>
</li>
<li>
<p>🍉它的顺序为每个线程是原子性的，每个线程内部操作执行顺序是和程序顺序相同的。</p>
</li>
<li>
<p>🍉实际上JMM采用的内存模型为保证线程原子性的同时对线程内部进行适当的重排序操作。</p>
</li>
</ul>
</li>
<li>
<p>8.没有同步过的程序：</p>
<ul>
<li>
<p>JMM保证未同步的程序读取值一定是被初始化过的，要么为之前写入的，要么为默认值。</p>
</li>
<li>
<p>🍉JMM不保证多线程和单线程之间的执行顺序相同，同时也不保证64位long型和double型的写操作具有原子性<br>
（因为64位数据实际上写入不是一步完成的，可能会被重排序），没有同步过的单线程是不用担心的，因为数据依赖性的关系不会出现数据丢失。</p>
</li>
</ul>
</li>
<li>
<p>9.volatile特性在内存上的理解：</p>
<ul>
<li>
<p>🍉要理解volatile特性，可以把volatile变量中读/写的操作看成对读/写操作加了同一个锁。</p>
</li>
<li>
<p>🍉当然，volatile实际上和锁没啥关系，上一章讲过，volatile在cpu层面上是直接使用lock命令，虽然和锁底层实现机理相同（锁的CAS操作也调用了lock）</p>
</li>
<li>
<p>🍉此处使用锁来做类比也是为了更好的理解volatile，在内存特性上，volatile的读和锁的获取有相同语义，volatile的写和锁的释放有相同语义（当volatile写入的时候，JMM会把该线程的本地内存刷新到主内存，当volatile读入的时候，会把线程本地内存置为无效，强制从主内存读取数据），</p>
</li>
<li>
<p>🍎volatile读写操作不止有原子性，也有先后顺序，最后的写操作永远先于任意的读操作（上章的cpu的总线锁和缓存锁就是为此存在的）</p>
</li>
<li>
<p>🍑书中写锁的内存可见性推导出volatile的内存可见性，容易被误解成volatile实现了锁的happens-before规则，但我认为书中原意是描写了volatile和锁的相似性<br>
（两者都是基于cpu的内存锁和缓存锁）</p>
</li>
</ul>
</li>
<li>
<p>10.JMM如何实现volatile的内存语义：</p>
<ul>
<li>
<p>JMM操作策略：</p>
<ul>
<li>1.volatile写永远在后面（volatile写入之前的操作不会被重排序到volatile后面）当第二个操作是volatile写的时候，第一个操作无论是什么都不能重排序</li>
<li>2.volatile读一定在前面（volatile读之后的操作不会被重排序到volatile前面）当第一个操作是volatile读的时候，第二个操作无论是什么都不能重排序</li>
<li>3.volatile写+读也不能排序，当第一个操作是volatile写，第二个操作是volatile读的时候，不能重排序</li>
</ul>
</li>
<li>
<p>JMM实际操作情况：</p>
<ul>
<li>
<p>🍉volatile写前面插入StoreStore屏障（禁止把v写和前面的写重排序），volatile写后面插入StoreLoad屏障（比较特殊禁止全部重排序，因为下面有可能有v读）</p>
</li>
<li>
<p>🍉volatile读前面插入LoadLoad屏障（禁止把前面的读和v读重排序），volatile读后面插入LoadStore屏障（禁止下面的写和v读重排序）</p>
</li>
<li>
<p>JMM的屏障操作可能不是最高效的，但十分保守。</p>
</li>
<li>
<p>🍎实际操作时JMM可以根据具体情况省略屏障，比如前后并没有读/写操作，这时屏障可以被省略来节省处理器内存开支</p>
</li>
</ul>
</li>
<li>
<p>JMM屏障的发展：</p>
<ul>
<li>
<p>事实上在JSR-133以前的java内存模型中，JMM允许v变量读写和普通读写的重排序，这样会导致不存在数据依赖的值（如v变量和普通变量之间）发生数据丢失。为了增强volatile的内存语义，专家组增加了上面写的那些屏障。直接导致了volatile的读写有了加锁和撤销锁的语义</p>
</li>
<li>
<p>🍎volatile只能保证数据的读写是原子性的，因为volatile就是强制让线程去主内存获取数据。例子：如果两个线程同时运行i++，同时获取i=0，那么结果还是1，并没有保证线程的安全性，线程有时可能并没有更新数据，线程就被切换了。</p>
</li>
<li>
<p>🍊重点：JMM实现volatile实际上是运用底层lock指令，lock指令中包含了cpu屏障技术，lock指令保证volatile读写操作的原子性和顺序，<br>
屏障技术仅仅阻止了重排序，但并不能阻止复杂操作的数据读取混乱，例如同时运行i++。</p>
</li>
<li>
<p>🍊任何带有lock前缀的指令以及CPUID等指令都有内存屏障的作用。</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>11.锁特性在内存上的理解：</p>
<ul>
<li>
<p>实际上</p>
</li>
<li>
<p>🍉当线程释放锁时，JMM会将线程的本地内存立刻刷新到主内存中（参照上一章的图片），其实线程释放锁就是volatile的写入（上面把锁和volatile一起讲解，现在能看明白了吧）。</p>
</li>
<li>
<p>🍉当线程获得锁时，JMM会把线程对应的本地内存置无效，并去主内存中读取数值，其实线程获得锁就是volatile的读取。</p>
</li>
<li>
<p>🍎线程1释放锁—线程2获取锁的过程就是线程1向线程2发送信息的过程（通过主内存）</p>
</li>
</ul>
</li>
<li>
<p>12.JMM如何实现锁：</p>
<ul>
<li>
<p>分析java中一个加锁类ReentrantLock的源代码：</p>
<ul>
<li>
<p>它调用一个lock()方法获取锁，unlock()方法释放锁。</p>
</li>
<li>
<p>🍉这个类的实现依赖于AQS（java同步器框架），原理为AQS会维护一个整形的volatile变量state。这个类分为公平锁和非公平锁</p>
</li>
<li>
<p>公平锁：在获取锁时首先读volatile变量，这时共享变量只有当前获取锁的线程可以看到。<br>
释放锁时写volatile变量，并且使共享变量对其他获取锁的线程可见。</p>
</li>
<li>
<p>非公平锁：释放锁时和公平锁操作相同。<br>
在获取锁的时候使用CAS方法更新state变量，CAS同时具有volatile读和写的内存语义。</p>
</li>
</ul>
</li>
<li>
<p>🍎通过这个类可以分析出，java中锁处理具有两种方式：<br>
*   1.通过volatile读写的内存语义上锁<br>
*   2.利用CAS进行上锁（CAS附带volatile的语义）</p>
</li>
</ul>
</li>
<li>
<p>13.CAS是如何实现的：</p>
<ul>
<li>
<p>首先，CAS同时具有volatile读和写的内存语义</p>
</li>
<li>
<p>比较和交换（Conmpare And Swap）是用于实现多线程同步的原子指令。 它将内存位置的内容与给定值进行比较，只有在相同的情况下，将该内存位置的内容修改为新的给定值。</p>
</li>
<li>
<p>CAS实际上是调用本地处理器中的C++代码，如果程序在多处理器上运行，c++代码执行带有lock前缀的cmpxchg。如果在单处理器上运行，就会省略lock前缀。</p>
</li>
<li>
<p>🍉在同一时刻，只会有一个线程更新数据成功</p>
</li>
</ul>
</li>
<li>
<p>14.concurrent包的实现：</p>
<ul>
<li>
<p>由于java的CAS和volatile运用了cpu上的原子指令，并且他们可以实现线程之间的通信，所以concurrent的通用化实现模式就诞生了。</p>
</li>
<li>
<p>实现模式：声明变量为volatile，使用CAS实现线程之间的同步，之后使用volatile的读和写还有CAS所具有的v内存语义实现线程的通信。</p>
</li>
<li>
<p>🍊🍊🍊重点：volatile只能保证数据读写的原子性，涉及到复杂操作，volatile并不能保证，所以这里使用CAS去更新。因为CAS每次更新前都要比较。</p>
</li>
<li>
<p>整体来看，concurrent包使用了底层CAS和volatile实现了AQS，非阻塞数据结构和原子变量类，进而实现其他高层类。</p>
</li>
</ul>
</li>
<li>
<p>15.final在内存上的理解：</p>
<ul>
<li>
<p>1.final的重排序规则：</p>
<ul>
<li>
<p>1.这两个操作不能重排序，对一个对象的final变量的写入———————把这个对象的引用进行赋值给另一个引用</p>
</li>
<li>
<p>2.这两个操作不能重排序，初次读一个包含final变量的引用———————初次读这个final变量</p>
</li>
<li>
<p>下面对这两个重排序禁止规则详细介绍：</p>
<ul>
<li>
<p>1.final写的重排序：<br>
JMM禁止把构造函数中的写final变量操作排序到构造函数外面。</p>
<p>🍉具体实现操作是JMM会在final变量写操作之后，构造函数结束的return之前，插入一个StoreStore屏障。这样就保证了构造函数执行完成前，final数据一定被初始化了。</p>
<p>🍉像不是final的变量，在构造函数中赋值写入的操作可能就会被重排序到读取的后面，导致数据丢失。</p>
</li>
<li>
<p>2.final读的重排序：<br>
JMM禁止读对象引用和读对象的final值重排序</p>
<p>（这两个操作实际上具有间接依赖关系，大部分处理器都不会重排序他们，但少部分处理器会重排序，这个规则就是为此存在的）</p>
</li>
</ul>
</li>
<li>
<p>🍎引用类型的final变量，和基本数据类型的重排序规则差不多，但是引用内部的数据初始化可能并不正确，<br>
比如数组类型，可能只能初始化大小，内部的值是否初始化不一定，因为会被重排序。</p>
</li>
<li>
<p>🍉还有一点，具有final变量的类，构造方法中不能出现this，这样会导致“溢出”，因为在构造方法结束之前就初始化了此对象，这样会导致屏障不能得到正确的使用。</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>16.final在cpu中的实现：</p>
<ul>
<li>
<p>🍉final实际上就是在final写后和构造的return前插入StoreStore屏障，在final读前插入LoadLoad屏障。</p>
</li>
<li>
<p>🍉在X86处理器中，因为X86不会对写-写作重排序，所以会忽略StoreStore屏障，同时final读对象引用实际上具有间接依赖关系，X86也不会处理这个操作。<br>
发现了吗，实际上X86对final没有进行任何操作。</p>
</li>
<li>
<p>🍉现在，final修饰的变量，只要保证被正确构造，在任意线程就能看到这个被构造函数初始化之后的值。</p>
</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java并发编程艺术第二章学习笔记📒]]></title>
        <id>https://wkf1115.github.io/post/java-bing-fa-bian-cheng-di-er-zhang-xue-xi-bi-ji/</id>
        <link href="https://wkf1115.github.io/post/java-bing-fa-bian-cheng-di-er-zhang-xue-xi-bi-ji/">
        </link>
        <updated>2020-04-21T07:44:57.000Z</updated>
        <content type="html"><![CDATA[<p>第二章：java并发机制的底层实现原理</p>
<p>核心观点：java源代码在编译后会变为java字节码，字节码被类加载器加载到jvm中，jvm执行字节码，最终转化为汇编指令在cpu上执行。<br>
所以在讨论并发时与cpu和jvm是分不开的。</p>
<p>1.volatile</p>
<ul>
<li>
<p>首先volatile是轻量级的synchronized，它主要保证  共享变量（也就是主内存的变量） 的可见性（其他线程能否读到这个值）。<br>
它没有加锁，没有线程上下文的切换，所以它的执行成本很低。</p>
<ul>
<li>
<p>1.定义：<br>
java语言允许线程访问共享变量，为了确保共享变量能够及时的更新，所以使用了volatile。<br>
换句话说，如果一个字段是volatile，那么JMM会确保所有线程使用该变量的值都是一致的。</p>
</li>
<li>
<p>2.底层实现：<br>
在汇编层面，volatile实际上是使用了Lock前缀的指令，这个Lock指令实际上是<br>
将缓存行中的指定数据写回到系统内存中，同时使其他cpu中缓存行中该数据的内存地址无效。</p>
<ul>
<li>
<p>为了说明这个问题，我们使用图片模拟cpu（线程）读写数据的情况。<br>
<img src="https://wkf1115.github.io/post-images/1587459567354.png" alt="" loading="lazy"></p>
</li>
<li>
<p>首先，cpu读取数据是从缓存行进行读取，同时缓存行与主内存进行数据交互，这样能够提升程序的运行速度，在单cpu（单线程）时，这样的运行方式没有问题。<br>
多cpu时，由于缓存行与主内存数据更新不及时，会导致数据更新丢失。<br>
例如：java程序启动时1线程和2线程从主内存中将a  = 1的地址读取到各自的缓存行。1线程中a++，2线程中缓存行中的a并没有变化，这就导致了数据更新丢失。<br>
<img src="https://wkf1115.github.io/post-images/1587459682774.png" alt="" loading="lazy"></p>
</li>
</ul>
<p>￼</p>
<ul>
<li>
<p>而volatile数据在更新时会向主内存写入数据，同时强制其他cpu读取数据A时去主内存中读取。<br>
这样就保证了数据A更新的原子性。</p>
</li>
<li>
<p>🍉把处理器缓存中数据写到内存中，以前的处理器采用总线锁定，现在大部分处理器会采用缓存锁定。锁定也是为了保证操作的原子性</p>
</li>
<li>
<p>🍉处理器通常使用嗅探技术来保证不同处理器之间数据在总线上的一致性。例如，如果嗅探到一个处理器来检测其他处理器打算改写内存地址，而这个地址处于共享状态，那么处理器会使它的缓存行无效。在下次强制执行缓存行填充（这就是缓存锁的大致原理）</p>
</li>
</ul>
</li>
<li>
<p>3.神奇的优化<br>
JDK7中LinkTransferQueue队列类中将想要储存的变量扩充了15个变量，一个变量4个字节，算上本身有64个字节。那么为什么扩充字节反而提高了速度呢？</p>
<ul>
<li>因为在很多处理器中高速缓存行是64个字节，如果头尾节点不足64字节大小，那么数据读取到高速缓存行时会将头尾节点读取到同一缓存行中，我们也清楚，队列出队和入队需要对头尾节点频繁操作，如果头节点被改变，根据volatile的数据一致性，cpu会锁定对应的缓存行。这会导致其他cpu无法访问尾节点拖慢运行速度。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>2.synchronized</p>
<ul>
<li>
<p>首先，对synchronized的运用实际上就是对锁的运用，java中任何一个对象都可以作为锁。</p>
<ul>
<li>
<p>1.java对象头：<br>
对象是否为锁的信息实际上是存在java对象头中的Mark Word里面，包括它的锁的类型。</p>
</li>
<li>
<p>2.锁的类型：<br>
偏向锁-&gt;轻量级锁-&gt;重量级锁</p>
<ul>
<li>
<p>1.偏向锁：<br>
在大部分情况下，程序中的锁总是由同一线程使用，如果这时重复的加锁解锁那么程序效率会浪费，为了让线程以更低的代价使用锁，偏向锁诞生了。</p>
<ul>
<li>
<p>那么偏向锁是什么呢？</p>
</li>
<li>
<p>当一个线程访问带锁的同步块并试图获取锁时，会在对象头中储存线程ID，以后在线程使用锁时实际验证对象头中的线程ID是否和自己的ID相同即可。</p>
</li>
<li>
<p>偏向锁实际上是出现线程竞争时才会发生变化。如果1线程持有偏向锁，此时2线程想要获取偏向锁，那么到下一次全局安全点时（此时没有java字节码执行），1线程会被暂停。如果1线程依然活着，要么2线程获取到偏向锁，要么偏向锁会升级。之后暂停的1线程会被唤醒。</p>
</li>
<li>
<p>🍉综上所述：偏向锁只是通过锁验证线程的ID，省时省力。</p>
</li>
</ul>
</li>
<li>
<p>2.轻量级锁：</p>
<ul>
<li>
<p>取锁过程：</p>
<ul>
<li>1.线程取锁时，会复制锁中对象头的mark word到线程的栈帧（线程中储存锁记录的空间）中</li>
<li>2.之后线程会使用CAS尝试把锁中对象头的mark word改为指向此线程的栈帧（锁记录）的指针
<ul>
<li>
<p>2.1.操作成功，代表此时锁没有线程使用。</p>
</li>
<li>
<p>2.2.操作失败，代表这个锁已被修改（CAS没有通过），此时由于是轻量级锁，此线程会选择进行自旋等待锁被释放，自旋一定次数会膨胀。</p>
</li>
<li>
<p>🍉持有锁的线程会把程序体执行完成再进行解锁。</p>
</li>
<li>
<p>🍉这里提到的自旋实际上就是等待一段时间，没有锁线程等待有锁的线程释放锁，等一会可以，如果时间太久了那么等待中的线程不干了就会掀桌。表现为进行膨胀，将锁升级为重量级。</p>
</li>
<li>
<p>🍍中间过程为拿到锁的线程执行方法体的过程🍍</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>解锁过程：</p>
<ul>
<li>1.轻量级锁解锁时，持有锁的线程会使用CAS把线程栈帧锁记录中的Mark Word副本换回锁中的对象头中
<ul>
<li>
<p>1.1成功代表没有竞争。这个锁依然为轻量级。</p>
</li>
<li>
<p>1.2失败了还说啥，锁膨胀为重量级，剩下的线程进行下一轮抢锁大战。</p>
<p>🍎如果锁在程序体执行过程中锁膨胀了，其他想要抢这个锁的线程都会进入阻塞状态直到锁被释放</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>3.重量级锁：</p>
<ul>
<li>
<p>真正的synchronized，直接将没有锁的线程进行阻塞，直到锁被释放。</p>
<ul>
<li>
<p>三个锁的优缺点：</p>
<p>1.偏向锁：<br>
优点：我们偏向锁，一天到晚除了快就是快，没别的。因为加锁解锁根本不需要额外的消耗，就是往对象头中加一串ID。<br>
缺点：如果这个锁被别人竞争了，那么锁进行撤销的过程需要额外的消耗。</p>
<pre><code>   🍉一个线程访问的环境就用偏向锁
</code></pre>
<p>2.轻量级锁：<br>
优点：竞争的线程实际上在自旋，如果锁被释放了，线程马上可以补上，响应速度相比重量级锁的阻塞线程要快。<br>
缺点：如果线程一直不放锁，那么自旋会消耗不必要的cpu。</p>
<pre><code>   🍉追求响应时间，同步块执行速度快的时候用轻量级锁
</code></pre>
<p>3.重量级锁：<br>
优点：和轻量级锁相比不自旋，就不会消耗cpu。<br>
缺点：线程阻塞，相应（再启动）时间缓慢。</p>
<pre><code>   🍉追求吞吐量，同步块执行速度较长的时候用重量级锁
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>3.原子操作的实现原理</p>
<ul>
<li>
<p>1.处理器层面实现原子操作：</p>
<ul>
<li>
<p>首先，处理器的基本内存操作都是原子性的，从系统内存读取或者写入一个字节都是原子的，也就是说当一个处理器读写一个字节时，其他处理器不能访问这个字节的内存地址。<br>
但是，复杂的内存操作处理器就不能自动保证了，例如跨总线宽度，多个缓存行，跨页表的操作。这时需要两个机制：总线锁和缓存锁。</p>
<ul>
<li>
<p>1.总线锁：<br>
就是上面volatile中提到的总线锁，使用汇编中的Lock命令触发cpu的LOCK#信号，当一个cpu输出此信号时，其他cpu会被阻塞，该cpu独享主内存，处理完该数据后其他cpu才会被恢复，这就保证了数据的原子性。但是同时，其他cpu被阻塞会导致不必要的开销。目前cpu会在某些时候使用缓存锁替代总线锁。</p>
</li>
<li>
<p>2.缓存锁：<br>
由于cpu是和内部的高速缓存进行数据交互（见volatile章节的图），那么cpu1在对内部的高速缓存中的数据A进行操作时，cpu的嗅探机制会触发，同时缓存了数据A的cpu2会强制让其高速缓存内部的数据A地址失效，使其重新去主内存中获取数据A。</p>
<p>🍉缓存锁虽好，但对某些不能存在缓存中的数据不起作用，因为人家压根就不在缓存里，并且，对跨缓存行的数据也不起作用。</p>
</li>
</ul>
<p>🍎我们可以看到，其实volatile就是完全利用了cpu的原子操作，实际上就是调用了Lock命令（里面包含了屏障，下一章会讲），cpu收到Lock命令后可以选择总线锁，也可以选择缓存锁。</p>
</li>
</ul>
</li>
<li>
<p>2.java中实现原子操作：</p>
<ul>
<li>
<p>1.循环CAS：</p>
<ul>
<li>
<p>java中的CAS操作实际上是利用了cpu中的CMPXCHG指令实现的，就是比较这个值有没有发生变化。在java中有java内置的原子操作类（AtomicBoolean，AtomicInteger，AtomicLong），这些类都使用了循环CAS。并且，上面提到的轻量级锁的自旋就是循环CAS</p>
</li>
<li>
<p>🍉同时循环CAS也有一些问题，例如常见的ABA问题，指的是一个值变了两次，但好像没有变化一样。这样CAS就检查不出变化。解决ABA问题的思路就是加上一个值（比如版本号），在值每次变化时都进行自增，这样可以保证值变化的唯一性。从JDK1.5开始，Atomic包中提供了解决ABA问题的类。</p>
</li>
<li>
<p>🍉第二个问题也是轻量级锁的问题，那就是自旋循环CAS带来的性能开销有点大。这个问题就要斟酌看待了。使用处理器的pause指令对循环时间进行控制。</p>
</li>
</ul>
</li>
<li>
<p>2.各种各样的锁：</p>
<pre><code>  偏向锁，轻量级锁，重量级锁。这三个锁没啥好说的，上面有详细的讲。
</code></pre>
</li>
</ul>
<p>🍎我们到这里可以了解到，无论是CAS，轻量级锁，重量级锁，还是CMPXCHG，都是基于cpu的Lock指令<br>
🍎lock指令有三个作用：第一个就是锁住总线（总线锁和缓存锁），第二个是禁止一部分重排序，第三个是将缓存数据刷到主内存中<br>
🍎注意，CMPXCHG也是基于总线锁和缓存锁的，不然CAS不会有原子性</p>
</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java并发编程艺术第一章学习笔记📒]]></title>
        <id>https://wkf1115.github.io/post/java-bing-fa-bian-cheng-yi-zhu-di-yi-zhang-xue-xi-bi-ji/</id>
        <link href="https://wkf1115.github.io/post/java-bing-fa-bian-cheng-yi-zhu-di-yi-zhang-xue-xi-bi-ji/">
        </link>
        <updated>2020-04-21T07:36:45.000Z</updated>
        <content type="html"><![CDATA[<p>第一章：关于多线程的简单介绍</p>
<p>1.多线程中并发执行不一定比串行执行更快，当并发执行累加操作不超过百万次时，速度会比串行执行累加操作要慢。</p>
<ul>
<li>
<p>造成这个情况因为线程有创建和上下文切换的开销。</p>
</li>
<li>
<p>解决这个情况的方法有<br>
1.无锁并发编程：比如通过数据的ID按照Hash算法分段，不同的线程处理不同的数据，来避免使用锁。<br>
2.CAS算法<br>
3.协程：在单线程中实现多任务的调度。在单线程中维持多个任务间的切换</p>
</li>
</ul>
<p>2.死锁：</p>
<ul>
<li>
<p>经典的例子：例如1线程运行在加A锁的方法中调用加B锁的方法，2线程运行在加B锁的方法中调用加A锁的方法，这会引起两个线程互相等待对方释放锁。</p>
</li>
<li>
<p>死锁的核心问题：但在现实中这样的代码不常见，更为常见的情况是1线程拿到锁后因为一些异常（死循环）没有释放掉锁。此时也会引起死锁。归根结底是对锁释放的错误。</p>
<ul>
<li>避免死锁的方法：<br>
1.避免一个线程同时获取多个锁<br>
2.一锁一资源，一一对应。尽量不要一对多和多对一。<br>
3.使用定时锁来替代实体锁机制。<br>
4.数据库锁中加锁和解锁必须在一个数据库连接里。</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
</feed>